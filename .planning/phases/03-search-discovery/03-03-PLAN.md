---
phase: 03-search-discovery
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - api/app/logging_config.py
  - api/app/metrics.py
  - api/app/middleware/logging_middleware.py
  - api/app/main.py
  - api/app/worker/embedding_worker.py
autonomous: true

must_haves:
  truths:
    - "All API requests produce structured JSON log lines with request_id, path, method, status_code, and duration"
    - "The embedding worker produces structured JSON log lines with trace_id, model, and status for every embedding attempt"
    - "Prometheus counters track total embeddings processed (by model and success/error status)"
    - "Prometheus histograms track embedding duration and search latency"
    - "GET /metrics returns Prometheus exposition format text"
    - "A structlog warning is emitted at worker startup when embedding_model_id in existing traces differs from the currently configured model"
  artifacts:
    - path: "api/app/logging_config.py"
      provides: "structlog configuration with JSON rendering and contextvars"
      contains: "merge_contextvars"
    - path: "api/app/metrics.py"
      provides: "Prometheus Counter and Histogram definitions for embeddings and search"
      contains: "embeddings_processed"
    - path: "api/app/middleware/logging_middleware.py"
      provides: "Request logging middleware binding request_id and timing"
      contains: "request_id"
  key_links:
    - from: "api/app/main.py"
      to: "api/app/logging_config.py"
      via: "configure_logging() called in lifespan startup"
      pattern: "configure_logging"
    - from: "api/app/main.py"
      to: "api/app/metrics.py"
      via: "GET /metrics endpoint serving Prometheus data"
      pattern: "/metrics"
    - from: "api/app/worker/embedding_worker.py"
      to: "api/app/metrics.py"
      via: "embeddings_processed counter and embedding_duration histogram"
      pattern: "embeddings_processed"
---

<objective>
Add structured logging (structlog with JSON rendering), Prometheus metrics (embedding and search counters/histograms), and request tracing middleware — making the system observable and debuggable in production.

Purpose: Without observability, debugging embedding failures, search performance issues, and production incidents is guesswork. This plan provides the instrumentation layer for both the API and the worker.
Output: Structured JSON logs on all requests and worker operations, Prometheus /metrics endpoint with embedding and search metrics, and embedding model drift detection at worker startup.
</objective>

<execution_context>
@/home/bitnami/.claude/get-shit-done/workflows/execute-plan.md
@/home/bitnami/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-search-discovery/03-RESEARCH.md
@.planning/phases/03-search-discovery/03-01-SUMMARY.md

# Files to modify
@api/app/main.py — lifespan for startup, router registration, health endpoint
@api/app/worker/embedding_worker.py — worker loop (from 03-01)
@api/app/routers/search.py — search endpoint (from 03-02, if available)
@api/app/middleware/__init__.py — existing middleware package
</context>

<tasks>

<task type="auto">
  <name>Task 1: Structlog config, Prometheus metrics, and logging middleware</name>
  <files>
    api/app/logging_config.py
    api/app/metrics.py
    api/app/middleware/logging_middleware.py
  </files>
  <action>
**1. Create `api/app/logging_config.py`:**

Configure structlog for JSON structured logging:

```python
import structlog

def configure_logging():
    """Configure structlog with JSON rendering and contextvars for request tracing."""
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,  # MUST be first — merges per-request context
            structlog.stdlib.add_log_level,
            structlog.stdlib.add_logger_name,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer(),
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
```

**2. Create `api/app/metrics.py`:**

Define all Prometheus metrics for the application:

```python
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from fastapi.responses import Response

# Embedding worker metrics
embeddings_processed = Counter(
    "commontrace_embeddings_processed_total",
    "Total embeddings generated",
    ["model", "status"],  # status: success | error | skipped
)

embedding_duration = Histogram(
    "commontrace_embedding_duration_seconds",
    "Time to generate one embedding",
    ["model"],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],
)

# NOTE: Search endpoint metrics (search_requests, search_duration) are defined
# directly in api/app/routers/search.py by Plan 03-02. Do NOT duplicate them here
# to avoid prometheus_client duplicate registration errors.

# HTTP request metrics (from middleware)
http_requests = Counter(
    "commontrace_http_requests_total",
    "Total HTTP requests",
    ["method", "path", "status_code"],
)

http_request_duration = Histogram(
    "commontrace_http_request_duration_seconds",
    "HTTP request duration",
    ["method", "path"],
    buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],
)


async def metrics_endpoint():
    """FastAPI endpoint handler that returns Prometheus metrics."""
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

Note: We are NOT using multiprocess mode for Prometheus. The API and worker share the same Python package but run in separate containers — each has its own /metrics. For this phase, only the API exposes /metrics. Worker metrics are logged via structlog and can be scraped from the worker container in a future phase.

**3. Create `api/app/middleware/logging_middleware.py`:**

Implement a Starlette middleware that:
- Generates a `request_id` (uuid4) per request
- Binds `request_id`, `path`, `method` to structlog contextvars
- Times the request
- Logs completion with `status_code` and `duration_ms`
- Increments `http_requests` counter and observes `http_request_duration` histogram

```python
import time
import uuid
import structlog
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response
from app.metrics import http_requests, http_request_duration

log = structlog.get_logger()

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next) -> Response:
        request_id = str(uuid.uuid4())

        # Clear and bind per-request context
        structlog.contextvars.clear_contextvars()
        structlog.contextvars.bind_contextvars(
            request_id=request_id,
            path=request.url.path,
            method=request.method,
        )

        start = time.monotonic()
        try:
            response = await call_next(request)
        except Exception:
            duration = time.monotonic() - start
            log.error("request_failed", duration_ms=round(duration * 1000, 2))
            raise

        duration = time.monotonic() - start
        status_code = response.status_code

        # Prometheus metrics
        # Normalize path to avoid high-cardinality labels (strip UUIDs)
        normalized_path = request.url.path
        http_requests.labels(method=request.method, path=normalized_path, status_code=str(status_code)).inc()
        http_request_duration.labels(method=request.method, path=normalized_path).observe(duration)

        # Structured log
        log.info(
            "request_completed",
            status_code=status_code,
            duration_ms=round(duration * 1000, 2),
        )

        # Add request ID to response headers for traceability
        response.headers["X-Request-ID"] = request_id
        return response
```
  </action>
  <verify>
1. `cd /home/bitnami/commontrace/api && uv run python -c "from app.logging_config import configure_logging; configure_logging(); print('OK')"` — structlog configured.
2. `cd /home/bitnami/commontrace/api && uv run python -c "from app.metrics import embeddings_processed, search_requests, metrics_endpoint; print('OK')"` — metrics importable.
3. `cd /home/bitnami/commontrace/api && uv run python -c "from app.middleware.logging_middleware import RequestLoggingMiddleware; print('OK')"` — middleware importable.
  </verify>
  <done>
Structlog configured with JSON rendering and contextvars. Prometheus metrics defined for embeddings, search, and HTTP requests. Request logging middleware produces structured log lines per request.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire observability into main.py, worker, and search endpoint</name>
  <files>
    api/app/main.py
    api/app/worker/embedding_worker.py
  </files>
  <action>
**1. Update `api/app/main.py`:**

Add to the lifespan function, at the very top of startup (before Redis init):
```python
from app.logging_config import configure_logging
configure_logging()
```

Add middleware registration AFTER `app = FastAPI(...)`:
```python
from app.middleware.logging_middleware import RequestLoggingMiddleware
app.add_middleware(RequestLoggingMiddleware)
```

Add the /metrics endpoint:
```python
from app.metrics import metrics_endpoint
app.get("/metrics")(metrics_endpoint)
```

Keep ALL existing code (lifespan Redis init, health check, all router registrations) unchanged.

**2. Update `api/app/worker/embedding_worker.py`:**

Add Prometheus metric instrumentation to the `process_batch` function. For each trace embedding:

- Before embed call: `start = time.monotonic()`
- On success:
  ```python
  embeddings_processed.labels(model=model_id, status="success").inc()
  embedding_duration.labels(model=model_id).observe(time.monotonic() - start)
  ```
- On EmbeddingSkippedError:
  ```python
  embeddings_processed.labels(model="none", status="skipped").inc()
  ```
- On other Exception:
  ```python
  embeddings_processed.labels(model=OPENAI_MODEL, status="error").inc()
  embedding_duration.labels(model=OPENAI_MODEL).observe(time.monotonic() - start)
  ```

Add imports: `import time`, `from app.metrics import embeddings_processed, embedding_duration`, and `from app.services.embedding import OPENAI_MODEL`.

Add `configure_logging()` call at the top of `run_worker()` to configure structlog for the worker process.

Add embedding drift detection at worker startup (inside `run_worker()`, before the main loop):
```python
# Drift detection: warn if existing traces used a different model
async with async_session_factory() as db:
    from sqlalchemy import func, select as sa_select
    result = await db.execute(
        sa_select(Trace.embedding_model_id, func.count())
        .where(Trace.embedding_model_id.is_not(None))
        .group_by(Trace.embedding_model_id)
    )
    model_counts = result.all()
    for model_id, count in model_counts:
        if model_id != OPENAI_MODEL:
            log.warning(
                "embedding_model_drift_detected",
                existing_model=model_id,
                current_model=OPENAI_MODEL,
                trace_count=count,
            )
```

**Note:** Search endpoint metric instrumentation (search_requests, search_duration) is handled by Plan 03-02, which defines and instruments these metrics directly in `api/app/routers/search.py`. No search.py modifications needed here.
  </action>
  <verify>
1. `grep -q 'configure_logging' /home/bitnami/commontrace/api/app/main.py && echo 'Logging configured in main'`.
2. `grep -q 'RequestLoggingMiddleware' /home/bitnami/commontrace/api/app/main.py && echo 'Middleware registered'`.
3. `grep -q '/metrics' /home/bitnami/commontrace/api/app/main.py && echo 'Metrics endpoint registered'`.
4. `grep -q 'embeddings_processed' /home/bitnami/commontrace/api/app/worker/embedding_worker.py && echo 'Worker metrics wired'`.
5. `grep -q 'embedding_model_drift' /home/bitnami/commontrace/api/app/worker/embedding_worker.py && echo 'Drift detection present'`.
  </verify>
  <done>
All observability wired: structlog configured at API and worker startup, request logging middleware active, Prometheus metrics instrumented on worker, /metrics endpoint exposed, embedding drift warning at worker startup. Search endpoint metrics are handled by Plan 03-02.
  </done>
</task>

</tasks>

<verification>
Phase 3 Plan 03 verification:
1. `api/app/logging_config.py` exists with `configure_logging()` using merge_contextvars
2. `api/app/metrics.py` exists with Counter + Histogram definitions
3. `api/app/middleware/logging_middleware.py` exists with request_id binding and duration timing
4. `main.py` calls `configure_logging()`, adds RequestLoggingMiddleware, serves /metrics
5. Worker embedding_worker.py increments embeddings_processed counter, observes embedding_duration
6. Worker logs drift warning when existing model IDs differ from configured model
7. Search endpoint metrics (search_requests, search_duration) are handled by Plan 03-02
</verification>

<success_criteria>
- Every HTTP request produces a structured JSON log line with request_id, path, method, status_code, duration_ms
- Every embedding operation updates Prometheus counters (success/error/skipped)
- GET /metrics returns valid Prometheus exposition format
- Worker detects and warns about embedding model drift at startup
- Search endpoint metrics handled by Plan 03-02 (no search.py modification in this plan)
- No existing functionality broken — all prior routes and behaviors unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/03-search-discovery/03-03-SUMMARY.md`
</output>
