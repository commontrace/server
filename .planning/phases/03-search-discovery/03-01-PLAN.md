---
phase: 03-search-discovery
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api/app/services/embedding.py
  - api/app/worker/__init__.py
  - api/app/worker/embedding_worker.py
  - api/app/config.py
  - api/pyproject.toml
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Traces with embedding IS NULL are automatically picked up by the worker and embedded within one poll interval"
    - "The EmbeddingService calls OpenAI text-embedding-3-small and returns a 1536-dimensional vector"
    - "When OPENAI_API_KEY is missing, the worker logs a warning and skips embedding (leaves embedding NULL) rather than crashing"
    - "The worker docker-compose service runs the embedding worker loop instead of the placeholder sleep command"
    - "Multiple worker instances do not double-embed the same trace (SKIP LOCKED prevents contention)"
  artifacts:
    - path: "api/app/services/embedding.py"
      provides: "EmbeddingService with OpenAI primary and graceful skip fallback"
      contains: "AsyncOpenAI"
    - path: "api/app/worker/embedding_worker.py"
      provides: "Async polling loop: claim batch with FOR UPDATE SKIP LOCKED, embed, store"
      contains: "skip_locked"
    - path: "api/app/worker/__init__.py"
      provides: "Worker package init"
    - path: "docker-compose.yml"
      provides: "Worker service running embedding_worker instead of placeholder"
      contains: "embedding_worker"
  key_links:
    - from: "api/app/worker/embedding_worker.py"
      to: "api/app/services/embedding.py"
      via: "EmbeddingService.embed(text)"
      pattern: "svc\\.embed"
    - from: "api/app/worker/embedding_worker.py"
      to: "api/app/database.py"
      via: "async_session_factory for DB sessions"
      pattern: "async_session_factory"
    - from: "api/app/worker/embedding_worker.py"
      to: "api/app/models/trace.py"
      via: "SELECT Trace WHERE embedding IS NULL, UPDATE embedding column"
      pattern: "Trace\\.embedding\\.is_\\(None\\)"
---

<objective>
Build the async embedding worker and EmbeddingService that automatically processes newly submitted traces, generating vector embeddings via OpenAI's text-embedding-3-small model and storing them in the traces.embedding column.

Purpose: Without embeddings, semantic search (Plan 03-02) has nothing to query. The worker is the data pipeline that converts text traces into searchable vectors.
Output: A running worker process that polls for unembedded traces, calls the OpenAI embeddings API, and stores the 1536-dimensional vectors — all using the existing database schema and pgvector registration from Phase 1.
</objective>

<execution_context>
@/home/bitnami/.claude/get-shit-done/workflows/execute-plan.md
@/home/bitnami/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-search-discovery/03-RESEARCH.md

# Critical prior art
@api/app/database.py — async_session_factory, pgvector register_vector already wired
@api/app/models/trace.py — Trace.embedding Vector(1536), embedding_model_id, embedding_model_version
@api/app/config.py — Settings class (needs OPENAI_API_KEY added)
@docker-compose.yml — worker service with placeholder sleep command
</context>

<tasks>

<task type="auto">
  <name>Task 1: EmbeddingService and worker config</name>
  <files>
    api/app/services/embedding.py
    api/app/config.py
    api/pyproject.toml
  </files>
  <action>
**1. Add dependencies to `api/pyproject.toml`:**

Run: `cd /home/bitnami/commontrace && uv add "openai>=1.0" "sentence-transformers>=3.0" "prometheus-client>=0.20" --package commontrace-api`

These three are needed across Phase 3 plans. Adding them all now avoids repeated lockfile updates.

**2. Add `openai_api_key` to `api/app/config.py`:**

Add to the Settings class:
```python
openai_api_key: str = ""
```

This reads `OPENAI_API_KEY` from the environment (Pydantic Settings auto-maps uppercase env vars). Empty string means "not configured" — the EmbeddingService checks this.

**3. Create `api/app/services/embedding.py`:**

Implement `EmbeddingService` class:

- Constructor: checks `settings.openai_api_key`. If empty, set `self._skip = True` and log a WARNING via structlog: "OPENAI_API_KEY not set — embedding worker will skip trace embedding. Traces will not appear in semantic search until API key is configured."
- Lazy-init `AsyncOpenAI` client (only created on first `.embed()` call).
- `async def embed(self, text: str) -> tuple[list[float], str, str]`:
  - If `self._skip` is True: raise a `EmbeddingSkippedError` (custom exception defined in the same file).
  - Call `client.embeddings.create(input=text, model="text-embedding-3-small", dimensions=1536)`.
  - Return `(response.data[0].embedding, "text-embedding-3-small", response.model)`.
  - Do NOT implement a local sentence-transformers fallback that zero-pads. Per research open question #1, the simplest correct behavior is to skip embedding when no API key is present. The `sentence-transformers` dependency is installed for potential future use but is not wired in this plan.

Constants at module level:
```python
OPENAI_MODEL = "text-embedding-3-small"
OPENAI_DIMENSIONS = 1536
```

Custom exception:
```python
class EmbeddingSkippedError(Exception):
    """Raised when embedding is skipped (no API key configured)."""
    pass
```

Import structlog for logging. Import settings from app.config. Import AsyncOpenAI from openai.
  </action>
  <verify>
`cd /home/bitnami/commontrace && uv run python -c "from app.services.embedding import EmbeddingService, EmbeddingSkippedError; print('OK')"` — must print OK without import errors. Run from the `api/` directory.
  </verify>
  <done>
EmbeddingService importable, raises EmbeddingSkippedError when no OPENAI_API_KEY, calls AsyncOpenAI when key present. All Phase 3 dependencies installed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Embedding worker polling loop and docker-compose wiring</name>
  <files>
    api/app/worker/__init__.py
    api/app/worker/embedding_worker.py
    docker-compose.yml
  </files>
  <action>
**1. Create `api/app/worker/__init__.py`:**

Empty file (package marker).

**2. Create `api/app/worker/embedding_worker.py`:**

Implement the polling worker loop with these components:

Module-level constants:
```python
POLL_INTERVAL_SECONDS = 5
BATCH_SIZE = 10
```

`async def process_batch(db: AsyncSession, svc: EmbeddingService) -> int`:
- Execute: `SELECT * FROM traces WHERE embedding IS NULL FOR UPDATE SKIP LOCKED LIMIT 10`
  - SQLAlchemy: `select(Trace).where(Trace.embedding.is_(None)).with_for_update(skip_locked=True).limit(BATCH_SIZE)`
- If no traces found, return 0.
- For each trace:
  - Build text: `f"{trace.title}\n{trace.context_text}\n{trace.solution_text}"`
  - Try `svc.embed(text)` — returns `(vector, model_id, model_version)`.
  - On success: `UPDATE traces SET embedding=vector, embedding_model_id=model_id, embedding_model_version=model_version WHERE id=trace.id`
    - Use `db.execute(update(Trace).where(Trace.id == trace.id).values(embedding=vector, embedding_model_id=model_id, embedding_model_version=model_version).execution_options(synchronize_session=False))`
  - Log: `log.info("embedding_stored", trace_id=str(trace.id), model=model_id)`
  - On `EmbeddingSkippedError`: log warning once and return 0 (no point continuing the batch — all will skip).
  - On other Exception: log error with trace_id, continue to next trace (don't re-raise — trace will be retried next poll).
- After all traces processed: `await db.commit()`
- Return count of traces processed.

`async def run_worker()`:
- Create `EmbeddingService()` instance.
- Log: `log.info("embedding_worker_started")`
- Infinite loop:
  - `async with async_session_factory() as db:`
    - `count = await process_batch(db, svc)`
    - If count > 0: log `batch_processed` with count.
  - Catch any outer Exception: log error, continue loop.
  - `await asyncio.sleep(POLL_INTERVAL_SECONDS)`

Module `__main__` guard:
```python
if __name__ == "__main__":
    import asyncio
    asyncio.run(run_worker())
```

Imports needed: `asyncio`, `structlog`, `sqlalchemy.select`, `sqlalchemy.update`, `app.database.async_session_factory`, `app.models.trace.Trace`, `app.services.embedding.EmbeddingService`, `app.services.embedding.EmbeddingSkippedError`.

**3. Update `docker-compose.yml`:**

Change the worker service command from the placeholder:
```yaml
command: >
  python -c "import asyncio; asyncio.run(asyncio.sleep(86400))"
```

To:
```yaml
command: >
  sh -c "alembic upgrade head && python -m app.worker.embedding_worker"
```

This ensures migrations are applied before the worker starts (avoiding Pitfall 7 from research). The `alembic upgrade head` is idempotent — if API already ran it, it's a no-op.

Also add a dependency on the api service being healthy to avoid race conditions:
```yaml
depends_on:
  postgres:
    condition: service_healthy
  redis:
    condition: service_healthy
```

Keep the existing depends_on (postgres + redis). Do NOT add `api: service_healthy` dependency because the api service does not have a healthcheck defined, and the worker only needs the database (not the API) to function.
  </action>
  <verify>
1. `cd /home/bitnami/commontrace && uv run python -c "from app.worker.embedding_worker import run_worker, process_batch; print('OK')"` — must print OK (run from api/).
2. `grep -q 'embedding_worker' /home/bitnami/commontrace/docker-compose.yml && echo 'Worker command updated'` — confirms docker-compose updated.
3. `grep -q 'skip_locked' /home/bitnami/commontrace/api/app/worker/embedding_worker.py && echo 'SKIP LOCKED present'` — confirms atomic claim pattern.
  </verify>
  <done>
Worker polling loop claims traces with FOR UPDATE SKIP LOCKED, embeds via EmbeddingService, stores vectors. Docker-compose worker service runs the real embedding worker with alembic migration pre-check.
  </done>
</task>

</tasks>

<verification>
Phase 3 Plan 01 verification:
1. `api/app/services/embedding.py` exists with EmbeddingService class
2. `api/app/worker/embedding_worker.py` exists with run_worker + process_batch
3. `docker-compose.yml` worker command references embedding_worker (not placeholder sleep)
4. `api/pyproject.toml` has openai, sentence-transformers, prometheus-client dependencies
5. Import check passes: `uv run python -c "from app.services.embedding import EmbeddingService; from app.worker.embedding_worker import run_worker"` from api/ dir
</verification>

<success_criteria>
- EmbeddingService callable with OpenAI API; gracefully skips when no API key
- Worker loop polls every 5 seconds, claims batches atomically, stores embeddings
- Docker-compose worker service is production-ready (not a placeholder)
- No import errors; all new code integrates with existing database.py and models
</success_criteria>

<output>
After completion, create `.planning/phases/03-search-discovery/03-01-SUMMARY.md`
</output>
