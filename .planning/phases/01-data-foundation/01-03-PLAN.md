---
phase: 01-data-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - api/alembic.ini
  - api/migrations/env.py
  - api/migrations/script.py.mako
  - api/migrations/versions/0000_enable_pgvector.py
  - api/migrations/versions/0001_initial_schema.py
  - api/fixtures/seed_fixtures.py
  - api/fixtures/sample_traces.json
autonomous: true

must_haves:
  truths:
    - "Alembic migrations run cleanly on a fresh database creating all 5 tables"
    - "pgvector extension is enabled as the very first migration step"
    - "HNSW index exists on traces.embedding column using vector_cosine_ops operator class"
    - "A trace row can be created via ORM with all required fields and defaults to pending status"
    - "Fixture data loads sample traces that are auto-validated (status=validated, is_seed=True)"
    - "Tags in fixture data are normalized via normalize_tag before insertion"
    - "docker compose up -d followed by fixture loading produces a database with queryable traces"
  artifacts:
    - path: "api/alembic.ini"
      provides: "Alembic configuration pointing to async env.py"
      contains: "sqlalchemy.url"
    - path: "api/migrations/env.py"
      provides: "Async Alembic env.py importing all models"
      contains: "run_async_migrations"
    - path: "api/migrations/versions/0000_enable_pgvector.py"
      provides: "First migration enabling pgvector extension"
      contains: "CREATE EXTENSION IF NOT EXISTS vector"
    - path: "api/migrations/versions/0001_initial_schema.py"
      provides: "Schema migration creating all tables and HNSW index"
      contains: "hnsw"
    - path: "api/fixtures/sample_traces.json"
      provides: "10-15 sample traces with realistic content for dev testing"
      min_lines: 50
    - path: "api/fixtures/seed_fixtures.py"
      provides: "Script to load fixture data into database"
      contains: "TraceStatus.validated"
  key_links:
    - from: "api/migrations/env.py"
      to: "api/app/models/__init__.py"
      via: "model imports for autogenerate"
      pattern: "from app\\.models"
    - from: "api/migrations/env.py"
      to: "api/app/config.py"
      via: "settings.database_url for connection"
      pattern: "settings\\.database_url"
    - from: "api/migrations/versions/0001_initial_schema.py"
      to: "api/migrations/versions/0000_enable_pgvector.py"
      via: "Alembic revision chain (depends_on)"
      pattern: "down_revision"
    - from: "api/fixtures/seed_fixtures.py"
      to: "api/app/models/trace.py"
      via: "creates Trace objects with TraceStatus.validated"
      pattern: "TraceStatus\\.validated"
    - from: "api/fixtures/seed_fixtures.py"
      to: "api/app/services/tags.py"
      via: "normalize_tag called before creating tags"
      pattern: "normalize_tag"
---

<objective>
Set up Alembic async migrations, create the initial schema migration (including pgvector HNSW index), and build the fixture data seeding system so developers get a working database with sample data in one command.

Purpose: This plan connects the ORM models (Plan 01) to a real database (Plan 02), proving the entire data foundation works end-to-end. Migrations create the schema, fixtures populate it with realistic test data.

Output: `alembic upgrade head` creates all tables with pgvector HNSW index. Seed script loads 10-15 sample traces. Database is queryable with realistic data.
</objective>

<execution_context>
@/home/bitnami/.claude/get-shit-done/workflows/execute-plan.md
@/home/bitnami/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-foundation/01-CONTEXT.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@.planning/phases/01-data-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Alembic async setup and initial migrations</name>
  <files>
    api/alembic.ini
    api/migrations/env.py
    api/migrations/script.py.mako
    api/migrations/versions/0000_enable_pgvector.py
    api/migrations/versions/0001_initial_schema.py
  </files>
  <action>
Set up Alembic with the async template pattern and create the initial migrations. Do NOT use `alembic init` interactively — create the files manually following the async template pattern from research (Pattern 4).

1. **`api/alembic.ini`**:
   - Set `script_location = migrations`
   - Set `sqlalchemy.url = postgresql+asyncpg://commontrace:commontrace@localhost:5432/commontrace` (overridden by env.py at runtime)
   - Standard Alembic config: prepend_sys_path = ., version_path_separator = os

2. **`api/migrations/env.py`** (async pattern from research):
   - Import `asyncio`, `async_engine_from_config`, `pool`
   - Import `from app.config import settings`
   - Import ALL models explicitly (research pitfall #2 — autogenerate needs this):
     ```python
     from app.models.base import Base
     from app.models.trace import Trace       # noqa: F401
     from app.models.user import User         # noqa: F401
     from app.models.vote import Vote         # noqa: F401
     from app.models.tag import Tag, trace_tags  # noqa: F401
     ```
   - Override DB URL: `config.set_main_option("sqlalchemy.url", settings.database_url)`
   - Set `target_metadata = Base.metadata`
   - Implement `run_migrations_offline()` for SQL generation
   - Implement `do_run_migrations(connection)` that calls `context.configure(connection=connection, target_metadata=target_metadata)` then `context.run_migrations()`
   - Implement `async def run_async_migrations()` using `async_engine_from_config` with `poolclass=pool.NullPool`
   - Implement `run_migrations_online()` that calls `asyncio.run(run_async_migrations())`
   - Follow the exact pattern from research section "Pattern 4: Alembic Async Migration Setup"

3. **`api/migrations/script.py.mako`**: Standard Alembic template for new revision files.

4. **`api/migrations/versions/0000_enable_pgvector.py`** (MUST be first migration — research pitfall #1):
   - revision: generate a deterministic hex string
   - down_revision: None
   - `upgrade()`: `op.execute("CREATE EXTENSION IF NOT EXISTS vector")`
   - `downgrade()`: `op.execute("DROP EXTENSION IF EXISTS vector")`
   - This MUST run before any migration that uses Vector columns

5. **`api/migrations/versions/0001_initial_schema.py`**:
   - revision: generate a deterministic hex string
   - down_revision: revision ID from 0000 migration
   - `upgrade()`:
     a. Create `users` table with all columns from User model
     b. Create `traces` table with all columns from Trace model, including `Vector(1536)` embedding column, ForeignKey to users.id
     c. Create `votes` table with all columns from Vote model, ForeignKeys to traces.id and users.id, UniqueConstraint on (trace_id, voter_id)
     d. Create `tags` table with all columns from Tag model
     e. Create `trace_tags` join table with composite PK
     f. Create HNSW index on traces.embedding:
        ```sql
        CREATE INDEX ix_traces_embedding_hnsw
        ON traces
        USING hnsw (embedding vector_cosine_ops)
        WITH (m = 16, ef_construction = 64)
        ```
        Use `vector_cosine_ops` (NOT vector_l2_ops — research pitfall #3: must match cosine distance queries in Phase 3)
     g. Create B-tree indexes: `ix_traces_status`, `ix_traces_contributor_id`, `ix_traces_created_at`, `ix_tags_name`
   - `downgrade()`: Drop all tables in reverse dependency order (trace_tags, votes, tags, traces, users), drop HNSW index

   IMPORTANT: Write this migration manually (not via autogenerate) because autogenerate cannot generate the HNSW index `CREATE INDEX ... USING hnsw` DDL or the `CREATE EXTENSION` in the prior migration. Autogenerate also cannot handle `Vector(1536)` column type without custom type comparison — manual is more reliable for the initial schema.

   Use `sa.Column("embedding", Vector(1536), nullable=True)` with `from pgvector.sqlalchemy import Vector` imported at the top.

After creating the files, verify by starting Docker services and running migrations:
```bash
cd /home/bitnami/commontrace
docker compose up -d postgres redis
# Wait for postgres healthy
sleep 10
# Run migrations from api/ directory against Docker postgres
cd api
DATABASE_URL="postgresql+asyncpg://commontrace:commontrace@localhost:5432/commontrace" uv run alembic upgrade head
```
  </action>
  <verify>
Start postgres via Docker Compose, then run Alembic migrations:
```bash
cd /home/bitnami/commontrace && docker compose up -d postgres redis
sleep 10
cd /home/bitnami/commontrace/api && DATABASE_URL="postgresql+asyncpg://commontrace:commontrace@localhost:5432/commontrace" uv run alembic upgrade head
```
Migrations should complete without error. Then verify tables exist:
```bash
docker compose exec postgres psql -U commontrace -c "\dt"
```
Should show: users, traces, votes, tags, trace_tags, alembic_version tables. Then verify HNSW index:
```bash
docker compose exec postgres psql -U commontrace -c "\di ix_traces_embedding_hnsw"
```
Should show the HNSW index.
  </verify>
  <done>Alembic async env.py configured, 2 migrations created (pgvector extension + full schema). All 5 tables created in PostgreSQL. HNSW index exists on traces.embedding with vector_cosine_ops. B-tree indexes on status, contributor_id, created_at, tag name. Migrations run cleanly on fresh database.</done>
</task>

<task type="auto">
  <name>Task 2: Fixture data and seed script</name>
  <files>
    api/fixtures/__init__.py
    api/fixtures/seed_fixtures.py
    api/fixtures/sample_traces.json
  </files>
  <action>
Create realistic fixture data and a seeding script per user decision (pre-loaded fixture data so developers can test immediately).

1. **`api/fixtures/__init__.py`**: Empty file.

2. **`api/fixtures/sample_traces.json`**: Create 10-15 sample traces covering common coding tasks. Each trace has:
   - `title`: Short descriptive title (like SO question title)
   - `context`: Markdown problem description (2-4 paragraphs, realistic)
   - `solution`: Markdown solution (code blocks, explanations)
   - `tags`: Array of tag strings (will be normalized by seed script)
   - `agent_model`: Optional agent model name
   - `agent_version`: Optional agent version

   Sample trace topics (domain-agnostic but coding-focused for v1 wedge):
   1. "Setting up FastAPI with async SQLAlchemy 2.0" (tags: python, fastapi, sqlalchemy)
   2. "Docker Compose healthcheck for PostgreSQL" (tags: docker, postgresql)
   3. "Handling CORS in FastAPI for frontend integration" (tags: python, fastapi, cors)
   4. "pgvector HNSW index configuration for cosine similarity" (tags: postgresql, pgvector, search)
   5. "Alembic async migrations with asyncpg driver" (tags: python, alembic, postgresql)
   6. "Redis caching pattern for API responses" (tags: python, redis, caching)
   7. "JWT authentication middleware in FastAPI" (tags: python, fastapi, auth)
   8. "Pydantic v2 model validation with custom validators" (tags: python, pydantic, validation)
   9. "GitHub Actions CI pipeline for Python with uv" (tags: python, ci, github-actions)
   10. "Error handling patterns in async Python" (tags: python, async, error-handling)
   11. "TypeScript React component with API data fetching" (tags: typescript, react, api)
   12. "PostgreSQL index optimization for query performance" (tags: postgresql, performance, indexing)

   Each trace should have realistic, helpful content — not lorem ipsum. Context should describe a real problem an agent would face. Solution should include working code.

3. **`api/fixtures/seed_fixtures.py`**: Async script that loads fixture data into the database.
   - Import `from app.database import async_session_factory`
   - Import `from app.models.trace import Trace, TraceStatus`
   - Import `from app.models.user import User`
   - Import `from app.models.tag import Tag`
   - Import `from app.services.tags import normalize_tag`
   - `async def seed()`:
     a. Check if seed data already exists (query for User with email="seed@commontrace.dev") — if exists, print "Already seeded" and return (idempotent)
     b. Create a seed user: `User(email="seed@commontrace.dev", display_name="CommonTrace Seed", is_seed=True)`
     c. Load `sample_traces.json`
     d. For each trace in fixtures:
        - Create `Trace(title=..., context_text=..., solution_text=..., status=TraceStatus.validated, is_seed=True, contributor_id=seed_user.id, agent_model=..., agent_version=...)` — seed traces are auto-validated per user decision
        - For each tag, normalize with `normalize_tag()`, then get-or-create the Tag row (query by name, create if not exists)
        - Append tags to trace.tags
     e. Commit
     f. Print summary: number of traces and tags created
   - `if __name__ == "__main__": asyncio.run(seed())`
   - Script runs via: `cd /home/bitnami/commontrace/api && DATABASE_URL="..." uv run python -m fixtures.seed_fixtures`

   IMPORTANT: Seed traces set `status=TraceStatus.validated` and `is_seed=True` per user decision (seed traces auto-validated, no confirmation process needed).

   IMPORTANT: Use `normalize_tag()` from `app.services.tags` for ALL tag creation (DATA-03 requirement). Do NOT insert raw tag strings.

   For get-or-create pattern with async SQLAlchemy: use `select(Tag).where(Tag.name == normalized_name)`, execute, check result. If None, create new Tag. Do NOT use `session.get_or_create` (doesn't exist in SQLAlchemy).
  </action>
  <verify>
With Docker postgres running and migrations applied, run the seed script:
```bash
cd /home/bitnami/commontrace/api && DATABASE_URL="postgresql+asyncpg://commontrace:commontrace@localhost:5432/commontrace" uv run python -m fixtures.seed_fixtures
```
Should print summary of traces and tags created. Then verify data:
```bash
cd /home/bitnami/commontrace && docker compose exec postgres psql -U commontrace -c "SELECT count(*) FROM traces WHERE status='validated'"
```
Should show 10+ rows. Then verify tags are normalized:
```bash
docker compose exec postgres psql -U commontrace -c "SELECT name FROM tags ORDER BY name"
```
All tags should be lowercase, no duplicates. Run seed script again — should print "Already seeded" (idempotent).
  </verify>
  <done>10-15 realistic sample traces exist in database with status=validated and is_seed=True. Tags are normalized (lowercase, trimmed, deduplicated). Seed script is idempotent. Developers can test queries immediately after docker compose up.</done>
</task>

<task type="auto">
  <name>Task 3: End-to-end validation of all phase success criteria</name>
  <files></files>
  <action>
Run the complete end-to-end verification sequence against Docker services to prove all 5 phase success criteria are met. This task does not create files — it validates the system.

**Verification sequence:**

1. **Start all services:**
   ```bash
   cd /home/bitnami/commontrace
   docker compose down -v  # Clean slate
   docker compose up -d
   # Wait for services to be healthy
   sleep 15
   ```

2. **SC-1: Trace row with all fields** — Verify via psql:
   ```sql
   SELECT column_name, data_type FROM information_schema.columns WHERE table_name='traces' ORDER BY ordinal_position;
   ```
   Must show: id, title, context_text, solution_text, embedding, embedding_model_id, embedding_model_version, status, trust_score, confirmation_count, contributor_id, agent_model, agent_version, metadata_json, is_seed, created_at, updated_at

3. **SC-2: Embedding model columns exist** — Verify:
   ```sql
   SELECT embedding_model_id, embedding_model_version FROM traces LIMIT 1;
   ```
   Columns must exist and be queryable (even if NULL for seed data).

4. **SC-3: Tag normalization** — Run a Python script that tests normalize_tag directly:
   ```python
   from app.services.tags import normalize_tag
   assert normalize_tag("  Python  ") == "python"
   assert normalize_tag("FASTAPI") == "fastapi"
   ```
   Also verify in database: `SELECT DISTINCT name FROM tags` — all lowercase.

5. **SC-4: Pending state + validation threshold** — Verify trace defaults:
   ```sql
   -- Seed traces should be validated
   SELECT count(*) FROM traces WHERE status='validated' AND is_seed=true;
   -- Regular trace should default to pending (test via INSERT)
   INSERT INTO traces (id, title, context_text, solution_text, status, trust_score, confirmation_count, contributor_id, created_at, updated_at)
   VALUES (gen_random_uuid(), 'Test', 'ctx', 'sol', 'pending', 0, 0,
           (SELECT id FROM users LIMIT 1), now(), now());
   SELECT status FROM traces WHERE title='Test';  -- should be 'pending'
   ```
   Also verify VALIDATION_THRESHOLD is in settings:
   ```python
   from app.config import settings
   assert settings.validation_threshold == 2
   ```

6. **SC-5: Migrations clean + Docker Compose** — Already verified by steps above. Additionally check:
   ```bash
   docker compose ps  # Should show 4 services running
   curl http://localhost:8000/health  # Should return {"status":"ok"}
   ```

If any check fails, diagnose and fix the issue in the relevant file (may need to touch files from Plan 01 or 02). Document any fixes in the summary.

After all checks pass, clean up the test trace:
```sql
DELETE FROM traces WHERE title='Test';
```
  </action>
  <verify>
All 5 success criteria pass:
1. traces table has all 17 columns
2. embedding_model_id and embedding_model_version are queryable columns
3. Tags in database are all lowercase
4. New traces default to pending status; validation_threshold is configurable at 2
5. Docker Compose shows 4 running services; migrations ran cleanly; health check returns 200
  </verify>
  <done>Complete end-to-end validation passes. All 5 phase success criteria verified against running Docker services. Database has pre-loaded fixture data. System is ready for Phase 2.</done>
</task>

</tasks>

<verification>
1. `alembic upgrade head` runs without errors on fresh database
2. All 5 tables exist: users, traces, votes, tags, trace_tags
3. pgvector extension is enabled (verified by Vector column creation)
4. HNSW index exists on traces.embedding with vector_cosine_ops
5. Seed data: 10+ validated traces with normalized tags
6. Seed script is idempotent (safe to run multiple times)
7. `docker compose up -d` brings up postgres, redis, api, worker (4 services)
8. `curl localhost:8000/health` returns 200
9. All phase success criteria SC-1 through SC-5 verified
</verification>

<success_criteria>
- Alembic migrations run cleanly on fresh database (SC-5)
- All 5 tables created with correct columns and constraints
- HNSW index uses vector_cosine_ops (not vector_l2_ops)
- 10+ seed traces with status=validated exist in database
- All tags lowercase and deduplicated
- docker compose up -d starts 4 healthy services
- Health check endpoint responds at localhost:8000
- Phase success criteria SC-1 through SC-5 all verified
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-03-SUMMARY.md`
</output>
